
loss_ = [0.04505,0.04155,0.03889,0.03619,0.034733,0.033233,0.03187,0.03017,0.02848,0.02728,0.026454,0.02579,0.02525,0.02410,0.02336,0.022336,0.021336,0.020554,0.020306,0.019733,0.0190928,0.019565,0.018913,0.01873,0.018181,0.018927,0.01804,0.018227, 0.0182259,
 0.017845, 0.01822, 0.01809,0.0178266,0.018108,0.01735,0.017529,0.017529,0.016145,0.015868,0.01619,0.0156783,0.015769,0.0158013,0.0156897,0.0157524,0.016016, 0.0157011,0.0158020, 0.015347,
0.0154266,0.0152977,0.015545,0.015600,0.015120, 0.01550968,0.01535735,0.015440,0.015044,0.0150383,0.0148298,0.01468526,0.01475952,0.014742,0.01467925, 0.0147694,0.0150240,0.014854,0.014645,
0.0148519,0.0145920,0.014742,0.0147802,0.01463,0.0148370,0.014763,0.014642,0.0143354,      0.014361024487894683, 0.01433,0.0144991,0.0141542,0.014894,0.01438,0.0144462,0.01443,0.01443,0.01443,
0.01443,0.0144563,0.01311801,0.0133187,0.013574118,0.013075,0.013440, 0.0132676,0.01334677,0.01322502,0.0131977,0.0134787809,0.01309557,0.0134549,0.0136820,0.01349349,0.0132976,0.01322941800,
0.01333634778,0.01353508,0.01302445,0.0131105,0.0136073,0.013181,0.0131839531,0.013111116,0.0126157,0.01289710,0.013251,0.0130492, 0.01299720,0.01292427
]

loss_1 = [1/364244000 * x for x in loss_]

# Original list
loss_2_ = [3.700263e-07, 3.8204084e-07,3.7226127149245546e-07,3.7093055548828845e-07,3.70451775296916e-07, 3.7754813641485403e-07,3.78205937455e-07, 3.697921344859305e-07,3.770222500438971e-07,
           3.809966925266611e-07,3.659919450564618e-07,3.564772850150442e-07,3.690317987719429e-07,3.5747077657988346e-07,3.6594186928936663e-07,3.6600499123493927e-07,3.735487785716306e-07,
            3.686924500381351e-07,3.668821026411792e-07,3.564772850150442e-07,3.667595409461596e-07,3.564772850150442e-07,3.566349503776635e-07,3.564772850150442e-07,3.639003523974807e-07
          ]

loss_2 = [1/10000 * x for x in loss_2_]

loss_3 = [4.083454060999081e-11,3.564772850150442e-11,3.485460610631602e-11,2.744801771182085e-11,2.487815818616387e-11,2.7973828672936477e-11,2.4903698463238855e-11,
          4.707039929957803e-11,2.744801771182085e-11,2.2358214940084256e-11,1.94921620e-11,1.8817845553538806e-11,1.8633894590e-11,2.0122768928715646e-11,2.7287505597164152e-11,
          2.487815818616387e-11,2.7973828672936477e-11,2.4903698463238855e-11,1.8633894590e-11,2.2358214940084256e-11,1.8633894590e-11,1.94921620e-11,1.8817845553538806e-11,
          1.8502481320520934e-11,2.0112768928715698e-11,1.6424637133680072e-11,1.5935070801099352e-11,1.8985252911231717e-11,1.567005522583425e-11,1.5787288343938558e-11,
          1.6073221990708094e-11,1.567005522583425e-11,1.659007860945593e-11,1.454978467411903e-11,1.5021623797761117e-11,1.6452994453487584e-11,1.567005522583425e-11,1.3695041998995348e-11,1.5651620075373347e-11,
          1.5064550715713822e-11,1.5221954328470733e-11,1.3481437525515818e-11,1.5507825590574162e-11,1.3347125815067422e-11,1.2208934950923923e-11,1.1506038126566437e-11,
          1.2686542788520327e-11,1.0955680216410515e-11,1.1184264798137343e-11,1.1338485285286734e-11,1.2686542788520327e-11,1.0955680216410515e-11,1.1184264798137343e-11,1.1338485285286734e-11, 1.3126540550625159e-11,1.2252837510740247e-11, 1.07611637053607e-11,
          1.0949558867606483e-11,1.1293128736734162e-11,1.1018595348319212e-11, 9.263518771507329e-12, 1.1224619309035314e-11,1.0728383459390756e-11,1.0854159493485819e-11,
          1.2080095087888866e-11,1.2984840881719167e-11, 2.0815978507999464e-11,1.1864541698349393e-11,1.1315325606721808e-11,1.1824810753422973e-11,1.0602663180e-11,1.3630488525370032e-11,
          1.1523109191605389e-11,1.0099638305320255e-11,1.2543712881890713e-11,1.0951461271750939e-11,1.0530971816680252e-11,1.082531218403397e-11,1.0826603199677744e-11,
          1.0042771925345698e-11,1.0525490875044471e-11,1.1175572809691562e-11,1.1936140685993365e-11,2.0489369067344524e-11,1.1654088245457534e-11,1.1322891864678383e-11,
          1.1667186904880842e-11,1.1935577630929457e-11,1.3656247233769133e-11,1.2005409279680942e-11,1.1796599614578523e-11,1.294043978660351e-11,1.2202929680400326e-11,
          1.9273797229660564e-11,1.0848627329562877e-11,1.1695656419454675e-11,1.105669720919902e-11,1.0848627329562877e-11,1.1796599614578523e-11,1.0530971816680252e-11,

          ]
# Multiply each element in the list by 36424.4

loss_history = loss_1 + loss_2 + loss_3
# Print results
print(0.01349349 / 0.00000037045177529)
print('loss', loss_history)
print('len of loss', len(loss_history))


import matplotlib.pyplot as plt

# Assuming you already have the loss history data
# loss_history = loss_1 + loss_2 + loss_3
print(0.01349349 / 0.00000037045177529)
print('loss', loss_history)
print('len of loss', len(loss_history))

# Find the x values for the specific y values
y_value_1 = 2.0122768928715646e-11
y_value_2 = 0.014837 * 1/364244000

# Find the closest values in the loss history
x_value_1 = min(range(len(loss_history)), key=lambda i: abs(loss_history[i] - y_value_1))
x_value_2 = min(range(len(loss_history)), key=lambda i: abs(loss_history[i] - y_value_2))
print("x_value_1: ", x_value_1)
print("x_value_2: ", x_value_2)


# Plotting the loss history
plt.figure(figsize=(10, 6))
plt.plot(loss_history, label='Testing Loss', color='blue', linewidth=2)

# Adding vertical lines
plt.axvline(x=x_value_1, color='red', linestyle='--', label=f'Molecular Accuracy = 1.0')
plt.axvline(x=x_value_2, color='green', linestyle='--', label=f'Molecular Accuracy = 0.9')

# Adding title and labels
plt.title('Loss History', fontsize=16)
plt.xlabel('Epoch', fontsize=14)
plt.ylabel('Loss', fontsize=14)

# Adding grid and legend
plt.grid(True)
plt.legend()

# Show the plot
plt.show()
# loss for 0.9 : 4.0734e-11
# loss for 1.0 : 2.0123e-11


'''
from day 3_1
validation ratio:    1.0
molecular acuracy:   0.8845
symbol accuracy:     0.997620

validation ratio:    1.0
molecular acuracy:   0.888
symbol accuracy:     0.997890152


validation ratio:    1.0
molecular acuracy:   0.8847
symbol accuracy:     0.99752


molecular acuracy:   0.88975
symbol accuracy:     0.997909

molecular acuracy:   0.89
symbol accuracy:     0.99776128459


molecular acuracy:   0.8975
symbol accuracy:     0.99756469

molecular acuracy:   0.884
symbol accuracy:     0.99776368

molecular acuracy:   0.908
symbol accuracy:     0.99819

molecular acuracy:   0.8805
symbol accuracy:     0.997720


molecular acuracy:   0.888
symbol accuracy:     0.9978220

molecular acuracy:   0.899
symbol accuracy:     0.99793


molecular acuracy:   0.904
symbol accuracy:     0.997644


loss: 0.015297768221016805 * 1/364244000
molecular acuracy:   0.890625
symbol accuracy:     0.99796659

loss: 0.014837 * 1/364244000
molecular acuracy:   0.9005
symbol accuracy:     0.9982051

loss: 0.0133187 * 1/364244000
molecular acuracy:   0.9085
symbol accuracy:     0.998555875


loss:  0.0132676 * 1/364244000
molecular acuracy:   0.9125
symbol accuracy:     0.99866809


loss:      0.013225021467885779* 1/364244000
molecular acuracy:   0.914
symbol accuracy:     0.99831720220

loss: 0.01349349 * 1/364244000
molecular acuracy:   0.919
symbol accuracy:     0.9967

loss: 2.0122768928715646e-11
molecular acuracy:   1.0
symbol accuracy:     1.0


'''


